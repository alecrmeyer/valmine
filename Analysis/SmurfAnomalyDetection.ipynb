{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to database\n",
    "\n",
    "import mysql.connector\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  password=\"password\",\n",
    "  database=\"valorant_tracker\"\n",
    ")\n",
    "cursor = mydb.cursor() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"SELECT * FROM stats\", mydb)\n",
    "print(\"Number of samples: \" + str(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_id'] = df['match_player_id'].str[36:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop([\"match_player_id\", \n",
    "\"user_id\", \n",
    "\"date\", \n",
    "\"rounds\", \n",
    "\"map\", \n",
    "\"result\", \n",
    "\"agent\",\n",
    "\"smurf\",\n",
    "\"current_rank\",], axis=1)\n",
    "\n",
    "#Should eventually take into account ability casts for each agent\n",
    "data = data.drop([\"grenade_casts\",\n",
    "\"ability_2_casts\",\n",
    "\"ability_1_casts\",\n",
    "\"ultimate_casts\"\n",
    "], axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with the raw data is difficult since it is not very gaussian. Will instead average all stats and work with a dataset of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(df[\"kills\"], df[\"deaths\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No validation set or test set being used to find optimal epsilon (should consider doing that, however, 0.000001 seems to work fairly well). After checking the accounts below, some of them appear to have two different users. One plays extremely well and the other plays poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data = df[[\"kills\", \"deaths\", \"user_id\"]]\n",
    "avg_data = sub_data.groupby(\"user_id\").mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "isof = IsolationForest(n_estimators=500, max_samples='auto', contamination=float(0.006))\n",
    "clf = isof.fit(avg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly = clf.predict(avg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anomaly = avg_data\n",
    "test_anomaly[\"anomaly\"] = anomaly.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = test_anomaly[(test_anomaly[\"anomaly\"] == -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_true = anomalies[(anomalies[\"anomaly\"] == -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_data[\"kills\"], avg_data[\"deaths\"], 'g.')\n",
    "\n",
    "plt.plot(anom_true[\"kills\"].to_numpy(), anom_true[\"deaths\"].to_numpy(), 'or',\n",
    "         markersize= 10,markerfacecolor='none', markeredgewidth=2)\n",
    "plt.xlabel(\"Kills\")\n",
    "plt.ylabel(\"Deaths\")\n",
    "plt.title(\"Kills Vs Deaths Outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_dim_data = df.drop([\"match_player_id\", \n",
    "\"date\", \n",
    "\"rounds\", \n",
    "\"map\", \n",
    "\"result\", \n",
    "\"agent\",\n",
    "\"smurf\",\n",
    "\"current_rank\",\n",
    "\"grenade_casts\",\n",
    "\"ability_2_casts\",\n",
    "\"ability_1_casts\",\n",
    "\"ultimate_casts\",], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_high_dim_data = high_dim_data.groupby(\"user_id\").mean()\n",
    "avg_high_dim_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "for col in avg_high_dim_data:\n",
    "    sns.histplot(avg_high_dim_data[col], color=\"grey\", label=\"100% Equities\", kde=True, stat=\"density\", linewidth=0)\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "isof = IsolationForest(n_estimators=500, max_samples='auto', contamination=float(0.006))\n",
    "clf = isof.fit(avg_high_dim_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_high_dim = clf.predict(avg_high_dim_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anomaly_high_dim = avg_high_dim_data\n",
    "test_anomaly_high_dim[\"anomaly\"] = anomaly_high_dim.tolist()\n",
    "anomalies_high_dim = test_anomaly_high_dim[(test_anomaly_high_dim[\"anomaly\"] == -1)]\n",
    "print(\"Anomalies flagged: \" + str(anomalies_high_dim.shape[0]))\n",
    "anom_true_high_dim = anomalies_high_dim[(anomalies_high_dim[\"anomaly\"] == -1)]\n",
    "anom_true_high_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_with_id = anom_true_high_dim.reset_index()\n",
    "for user in anom_with_id[\"user_id\"]:\n",
    "    df.loc[df[\"user_id\"] == user, 'smurf'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smurf_count = df.loc[df[\"smurf\"] == 1].shape[0]\n",
    "legit_count = df.loc[df[\"smurf\"] == 0].shape[0]\n",
    "print(\"Number of users flagged as a smurf: \" + str(smurf_count))\n",
    "print(\"Number of users not flagged: \" + str(legit_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smurf_ratio = smurf_count / (smurf_count + legit_count)\n",
    "print(\"Smurf percentage: %.2f%%\" % (smurf_ratio * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "df_majority = df[df[\"smurf\"]==0]\n",
    "df_minority = df[df[\"smurf\"]==1]\n",
    "\n",
    "df_minority_upsampled = resample(df_minority,\n",
    "                                replace=True,\n",
    "                                    n_samples=legit_count,\n",
    "                                    random_state=111)\n",
    "\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "df_upsampled[\"smurf\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Running the upsampled data through a simple random forest model\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_upsampled = df_upsampled.dropna()\n",
    "df_upsampled = df_upsampled.sample(frac=1)\n",
    "y = np.array(df_upsampled['smurf'])\n",
    "X = df_upsampled.drop(['smurf', 'date', 'match_player_id', 'user_id'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = X[X.select_dtypes(exclude=['float64', 'int64', 'datetime64[ns]']).columns]\n",
    "for feature in numeric:\n",
    "    dummies = pd.get_dummies(X[feature])\n",
    "    X = pd.concat([X, dummies], axis=1)\n",
    "    X = X.drop([feature], axis=1)\n",
    "print(X.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Using Random Forest because we have a very large data set. The main drawback is interprtability would be usefull to see where and why certain splits happen. However, we do have access to feature importance since we used Random Forest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "rf = RandomForestClassifier(n_estimators=100, max_features=9)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "rf_predict=rf.predict(X_test)\n",
    "\n",
    "# #print confusion matrix and accuracy score\\n\",\n",
    "rf_conf_matrix = confusion_matrix(y_test, rf_predict)\n",
    "rf_acc_score = accuracy_score(y_test, rf_predict)\n",
    "print(rf_conf_matrix)\n",
    "accuracy = rf_acc_score*100\n",
    "print(accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The radom forest model above takes about 23 minutes to execute so lets save it in a joblib dump to reuse. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(rf, \"./Models/smurf_random_forest.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_loaded = joblib.load(\"./Models/smurf_random_forest.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df =  pd.DataFrame()\n",
    "imp_df[\"feature\"] = X_train.columns\n",
    "imp_df[\"importance\"] = rf.feature_importances_\n",
    "imp_df_sorted = imp_df.sort_values(by=[\"importance\"])\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.barh(imp_df_sorted[\"feature\"], imp_df_sorted[\"importance\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is very interesting and provides some more insight on what attributes can be focused on to detect smurfs. Before it was assumed that kills, deaths and headshot percentage would be the most important traits for smurf detection but as we can see here 'econ_rating', 'damage_recieved', 'ability_1_casts' are also considered very important features. It is also interesting to see how different agent and ranks differ importance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b6ddfb2dcac28d85c50c41a252d540a7fa8764b66ffa4029a84ab1fab305572"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
